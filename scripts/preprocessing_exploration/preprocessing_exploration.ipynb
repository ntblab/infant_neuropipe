{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform univariate analyses exploring preprocessing decisions\n",
    "\n",
    "This code should execute and recreate the main images reported in the manuscript. The figures should be identical but the statistics may differ slightly due to the random nature of bootstrap resampling.\n",
    "\n",
    "To make this script work, edit the file paths in the cell below to go to the two directories needed: \n",
    "1. The main path of the project directory (the cloned neuropipe repo)\n",
    "2. The place where the data for the methods is stored (downloaded from Dryad)\n",
    "\n",
    "If these are set up correctly then you should be able to run all cells\n",
    "\n",
    "Table of contents:\n",
    ">[Set up](#setup)  \n",
    ">[Plot voxelwise results](#wholebrain)  \n",
    ">[Plot ROI bar plots](#rois)  \n",
    ">[Plot different preprocessing ROIs for different parameters](#explore)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "proj_path = './infant_neuropipe'\n",
    "preloaded_path = '%s/data/methods_data/' % proj_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up<a id='setup'></a>\n",
    "\n",
    "Run the cells below to set up the analysis. This sets up the modules in the first cell, defines the parameters for plotting in the second, and then makes all the needed functions in the third (this is a big cell, keep scrolling!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import fcluster, linkage, dendrogram\n",
    "import sys\n",
    "import os\n",
    "from nilearn import plotting\n",
    "import scipy.spatial.distance as sp_distance\n",
    "from scipy import stats, ndimage\n",
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import pandas as pd\n",
    "import glob\n",
    "from scipy.ndimage.morphology import binary_dilation\n",
    "import matplotlib.style\n",
    "import matplotlib as mpl\n",
    "import nibabel\n",
    "mpl.style.use('seaborn-poster')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make additional paths\n",
    "output_mask_path = proj_path + 'results/preprocessing_exploration/matplotlib_plots/'\n",
    "mask_path = preloaded_path + '/ROIs/'\n",
    "\n",
    "os.system('mkdir -p %s' % output_mask_path)\n",
    "\n",
    "occipital_filename = mask_path + 'occipital_MNI_1mm.nii.gz'\n",
    "A1_filename = mask_path + 'A1_MNI_1mm.nii.gz'\n",
    "V1_filename = mask_path + 'V1_MNI_1mm.nii.gz'\n",
    "LOC_filename = mask_path + 'LOC_MNI_1mm.nii.gz'\n",
    "cut_coords=[19, -8]#[12, -9] # [17, 10]  # [23, -3]\n",
    "\n",
    "V1_color = np.array((70, 157, 49)) / 255\n",
    "LOC_color = np.array((38, 92, 124)) / 255\n",
    "A1_color = np.array((102, 102, 102)) / 255\n",
    "\n",
    "bar_plot_color = np.array((205, 152, 248)) / 255\n",
    "\n",
    "heatmap_cmap = 'inferno'\n",
    "\n",
    "#Make the ROI masks color maps\n",
    "V1_color_map = ListedColormap(list(V1_color) + [0.5])\n",
    "A1_color_map = ListedColormap(list(A1_color) + [0.5])\n",
    "LOC_color_map = ListedColormap(list(LOC_color) + [0.5])\n",
    "\n",
    "# Do you want to make nans go to zero, depicting it with a dotted line\n",
    "set_nans_to_zero = 1\n",
    "\n",
    "# Specify various plotting parameters\n",
    "linecolor = [0.8, 0.8, 0.8]\n",
    "face_color = LOC_color\n",
    "face_color_alt = 'white'\n",
    "linestyle = '-'\n",
    "markeredgewidth = 2 # How think is the perimeter of the marker\n",
    "linewidth = 1 # How wide should the lines be connecting points\n",
    "bg_bar_color = [0.7, 0.7, 0.7]\n",
    "markersize=7 # How big are the markers\n",
    "bee_swarm_spacing = markersize * 1.1 # How far apart are the beeswarm markers\n",
    "alpha = 0.5\n",
    "\n",
    "# How do you want to categorize the age bands\n",
    "age_bands = np.asarray([[0, 12], [12, 18], [18, np.inf]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a vector of brain voxels into a volume by unwrapping according to the mask\n",
    "def vector_2_vol(mask_name,\n",
    "                 brain_mask,\n",
    "                vector,\n",
    "                output_name):\n",
    "\n",
    "    # Load in the brain data for reference\n",
    "    brain_nii = nib.load(mask_name)\n",
    "    vol = np.zeros(brain_nii.shape)\n",
    "\n",
    "    # Get the list of nonzero voxel coordinates\n",
    "    coords = np.where(brain_mask)\n",
    "\n",
    "    # Map the ISC data into brain space\n",
    "    vol[coords] = vector\n",
    "\n",
    "    # Return or save the ISC data as a volume\n",
    "    out_nifti = nib.Nifti1Image(vol, brain_nii.affine, brain_nii.header)\n",
    "    if output_name is not None:\n",
    "       nib.save(out_nifti, output_name)\n",
    "    \n",
    "    return out_nifti\n",
    "\n",
    "# Convert the correlation into a p value\n",
    "def corr2p(N, r):        \n",
    "    se = 1 / np.sqrt(N - 3)  # The standard error of a fisher transform distribution\n",
    "    p_val = stats.t.pdf(r / se, df=N - 2)\n",
    "    return p_val\n",
    "\n",
    "def plot_df_line(df, bar_color=bar_plot_color):\n",
    "    # Plot the data frame based on the text file \n",
    "    \n",
    "    # What are the column names\n",
    "    conditions = df.columns[1:]\n",
    "    \n",
    "    # What is the data array\n",
    "    data_array = df.values\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    # Plot the data mean with a bar plot\n",
    "    plt.bar(np.arange(len(conditions)), np.nanmean(data_array[:, 1:], 0), color=bar_color)\n",
    "    plt.errorbar(np.arange(len(conditions)), np.nanmean(data_array[:, 1:], 0), np.nanstd(data_array[:, 1:], 0) / np.sqrt(data_array[:, 1:].shape[0]), linestyle='none', color=np.asarray(bar_color) * 0.75)\n",
    "    \n",
    "    # Set to zero so it is just a line plot\n",
    "    markersize = 0\n",
    "    \n",
    "    # Plot the ppt data\n",
    "    for ppt_counter in range(data_array.shape[0]):\n",
    "        \n",
    "        # Pull outdata for this condition\n",
    "        data_ppt = data_array[ppt_counter, :]\n",
    "        \n",
    "        # What is the participant age\n",
    "        age = data_ppt[0]\n",
    "        \n",
    "        if age > age_bands[0, 0] and age <= age_bands[0, 1]:\n",
    "            fillstyle='none'\n",
    "        elif age > age_bands[1, 0] and age <= age_bands[1, 1]:\n",
    "            fillstyle='bottom'\n",
    "        elif age > age_bands[2, 0] and age <= age_bands[2, 1]:\n",
    "            fillstyle='full'\n",
    "        \n",
    "        # Pull out the condition data\n",
    "        data_condition = data_array[ppt_counter, 1:]\n",
    "        \n",
    "        # Do you want to make NaNs go to zero (because they were excluded)\n",
    "        if set_nans_to_zero == 1:\n",
    "            plt.plot(np.nan_to_num(data_condition), linestyle='--', color='k', linewidth=linewidth / 2) # Put a dotted line beneath all plots so that it will continue if not show elsewhere\n",
    "\n",
    "        # Plot the data\n",
    "        if fillstyle is 'none':\n",
    "            plt.plot(data_condition, color=linecolor, markeredgecolor=bg_bar_color, markerfacecolor='None', linestyle=linestyle, marker='o', markersize=markersize, markeredgewidth=markeredgewidth, linewidth=linewidth)\n",
    "        else:\n",
    "            plt.plot(data_condition, color=linecolor, markeredgecolor=bg_bar_color, markerfacecolor=bg_bar_color, linestyle=linestyle, marker='o', markersize=markersize, markerfacecoloralt='None', fillstyle=fillstyle, markeredgewidth=markeredgewidth, linewidth=linewidth)\n",
    "    \n",
    "    # Remove the box around the figure\n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    plt.xticks(np.arange(len(conditions)), list(conditions), rotation=45)\n",
    "    plt.xlim([-0.5, len(conditions) - 0.5])\n",
    "     \n",
    "#     # Run the one way anova on the data\n",
    "#     arrays = []\n",
    "#     for i in range(1, df.values.shape[1]):\n",
    "#         arrays += [list(df.values[np.isnan(df.values[:, i]) == False, i])]\n",
    "#     f_stats = stats.f_oneway(*arrays)\n",
    "\n",
    "#     # Get the degrees of freedom\n",
    "#     k = df.values.shape[1]\n",
    "#     N = np.sum(np.isnan(df.values[:, 1:]) == False)\n",
    "#     df_b = k - 1\n",
    "#     df_w = N - k\n",
    "\n",
    "#     print('F(%d,%d)=%0.3f, p=%0.3f' % (df_b, df_w, f_stats.statistic, f_stats.pvalue))\n",
    "\n",
    "def plot_bar(df, colors, plot_SE=True):\n",
    "\n",
    "    # What are the column names\n",
    "    conditions = df.columns[1:]\n",
    "    \n",
    "    # What is the data array\n",
    "    data_array = df.values\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    # Plot the data mean with a bar plot\n",
    "    plt.bar(np.arange(len(conditions)), np.nanmean(data_array[:, 1:], 0), color=colors)\n",
    "    \n",
    "    # Do you want to plot the SE?\n",
    "    if plot_SE is True:\n",
    "        \n",
    "        error_ROI = np.nanstd(data_array[:, 1:], 0) / np.sqrt(data_array[:, 1:].shape[0])\n",
    "        plt.errorbar(np.arange(len(conditions)), np.nanmean(data_array[:, 1:], 0), np.vstack((error_ROI, error_ROI)), linestyle='none', color=np.asarray(colors) * 0.5)\n",
    "    else:\n",
    "        \n",
    "        # Plot each individual confound\n",
    "        for roi_counter in range(1, data_array.shape[1]):\n",
    "            lower_bound, upper_bound = randomise_diff(data_array[:, roi_counter], return_percentile=True)\n",
    "            plt.errorbar(roi_counter - 1, np.nanmean(data_array[:, roi_counter], 0), np.vstack((lower_bound, upper_bound)), linestyle='none', color=np.asarray(colors[roi_counter - 1]) * 0.75)\n",
    "        \n",
    "        \n",
    "    # Remove the box around the figure\n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    plt.xticks(np.arange(len(conditions)), list(conditions), rotation=45)\n",
    "    plt.xlim([-0.5, len(conditions) - 0.5])\n",
    "    \n",
    "    # Set the ylim depending on the type\n",
    "    if 'proportion_sig_voxels' in DV:\n",
    "        plt.ylim(y_range)\n",
    "        plt.yticks(y_ticks)\n",
    "        plt.ylabel('Prop. sig task vs rest')\n",
    "        plt.hlines(1 - p_val, -1, 3, 'k', linestyles='dashed', linewidth=1)\n",
    "\n",
    "        \n",
    "def plot_column(df, ylim=[0, 1]):\n",
    "    \n",
    "    # Set the size\n",
    "    plt.figure(figsize=(1.5, 2.4))\n",
    "    \n",
    "    # Cycle through participants\n",
    "    conditions = df.columns[1:]\n",
    "    for ppt_counter in range(df.shape[0]):\n",
    "        \n",
    "        # Pull out the ppt age\n",
    "        age = df['Ppt_age'][ppt_counter]\n",
    "    \n",
    "        # Sort the ppts into age bins\n",
    "        if age > age_bands[0, 0] and age <= age_bands[0, 1]:\n",
    "            age_color = 0\n",
    "        elif age > age_bands[1, 0] and age <= age_bands[1, 1]:\n",
    "            age_color = 1 \n",
    "        elif age > age_bands[2, 0] and age <= age_bands[2, 1]:\n",
    "            age_color = 2 \n",
    "            \n",
    "        # Cycle through the ROIs\n",
    "        ROI_vec = []\n",
    "        for roi_counter, ROI in enumerate(conditions):\n",
    "            ROI_vec += [df[ROI][ppt_counter]]\n",
    "        \n",
    "        # Plot the data acros ROIs\n",
    "        plt.plot([0, 1, 2], ROI_vec, c='k', linewidth=0.5)\n",
    "        \n",
    "    # Remove the box around the figure\n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    plt.xticks(np.arange(len(conditions)), list(conditions))\n",
    "    plt.xticks([0, 1, 2], ['V1', 'LOC', 'A1'])\n",
    "    plt.xlim([-0.5, len(conditions) - 0.5])\n",
    "    \n",
    "    # Plot the ylim if it is provided\n",
    "    if len(ylim) > 0:\n",
    "        plt.ylim(ylim)\n",
    "        plt.yticks([0, 1.0])\n",
    "        plt.hlines(1 - p_val, -1, 3, 'k', linestyles='dashed', linewidth=1)\n",
    "    \n",
    "    \n",
    "def plot_sig_map(overall_task_based_name, output_name, threshold, N=None, ROIs=['V1', 'A1']):\n",
    "    \n",
    "    # Assume this will be 1 unless changed below\n",
    "    vmax = 1\n",
    "    \n",
    "    # If the data is just the raw t stat then take the threshold as a p value and convert it into an uncorrected tstat threshold\n",
    "    if (overall_task_based_name.find('tstat') > 0) and threshold < 1 :\n",
    "        threshold = stats.t.ppf(threshold, N - 1)\n",
    "        print('Setting t threshold to %0.2f' % threshold)\n",
    "\n",
    "    vmax = 5\n",
    "        \n",
    "    # Load the participant in\n",
    "    nii = nib.load(overall_task_based_name)\n",
    "    \n",
    "    # If you are using a p value threshold then change the range of values to show the full colormap\n",
    "    if vmax == 1:\n",
    "        overall_task_based = nii.get_data()\n",
    "\n",
    "        # Pull out the data and do a hacky step to make the color range equivalent to the range from threshold to 1\n",
    "        overall_task_based = (overall_task_based - threshold) * (1 / (1 - threshold))\n",
    "        overall_task_based[overall_task_based<=0] = 0\n",
    "        print('The range is actually %0.2f to 1' % threshold)\n",
    "\n",
    "        nii = nib.Nifti1Image(overall_task_based.astype('float'), nii.affine)\n",
    "        \n",
    "        # Set to none since you have manually done this\n",
    "        threshold = None\n",
    "\n",
    "    fig = plotting.plot_stat_map(nii, \n",
    "                                 vmax=vmax, \n",
    "                                 cmap=heatmap_cmap,\n",
    "                                 draw_cross=False,\n",
    "                                 cut_coords=cut_coords,\n",
    "                                 display_mode='xz',\n",
    "                                 colorbar=True,\n",
    "                                 threshold=threshold,\n",
    "                                 symmetric_cbar=True,\n",
    "                                )\n",
    "    \n",
    "    plt.savefig(output_name[:-4] + '_without_roi.svg')\n",
    "\n",
    "    # Add the ROI to this\n",
    "    #fig.add_overlay(occipital_filename, cmap=occipital_color_map)\n",
    "    if 'V1' in ROIs:\n",
    "        fig.add_contours(V1_filename, cmap=V1_color_map, linewidths=1, alpha=alpha)\n",
    "    if 'A1' in ROIs:\n",
    "        fig.add_contours(A1_filename, cmap=A1_color_map, linewidths=1, alpha=alpha)\n",
    "    if 'LOC' in ROIs:\n",
    "        fig.add_contours(LOC_filename, cmap=LOC_color_map, linewidths=1, alpha=alpha)\n",
    "    \n",
    "    fig.add_overlay(nii, \n",
    "                    threshold=threshold, \n",
    "                    cmap=heatmap_cmap,\n",
    "                    vmax=vmax)\n",
    "    plt.savefig(output_name)\n",
    "     \n",
    "        \n",
    "def file_name_extractor(ppt_id, aggregate_type, feat_name):\n",
    "    \n",
    "    ppt_data = pd.read_csv(preloaded_path + 'participant_age.txt', header=None, sep='\\t', names=['neuropipe_name', 'age', 'cohort'])\n",
    "    cohort_ppts = []\n",
    "    for ppt in ppt_data['neuropipe_name']:\n",
    "        cohort = ppt_data[ppt_data['neuropipe_name'] == ppt]['cohort'].values[0]\n",
    "        if cohort == ppt_id:\n",
    "            cohort_ppts += [ppt_data[ppt_data['neuropipe_name'] == ppt]['neuropipe_name'].values[0]]\n",
    "\n",
    "    if aggregate_type == 'concat':\n",
    "        file_names = []\n",
    "        for ppt in cohort_ppts:\n",
    "            file_names += glob.glob('%s/stat_maps_session/%s/%s*.nii.gz' % (preloaded_path, feat_name, ppt))\n",
    "\n",
    "    elif aggregate_type == 'all':\n",
    "        file_names = []\n",
    "        for ppt in cohort_ppts:\n",
    "            file_names += glob.glob('%s/stat_maps_runs/%s/%s*.nii.gz' % (preloaded_path, feat_name, ppt))\n",
    "            \n",
    "    return file_names\n",
    "        \n",
    "    \n",
    "def load_each_subject(feat_name, aggregate_type, ppt_stem, ROIs, DV):\n",
    "    \n",
    "    # Cycle through the ROIs\n",
    "    df = pd.DataFrame()\n",
    "    for ROI_counter, ROI in enumerate(ROIs):\n",
    "    \n",
    "        # Get the file names\n",
    "        files = file_name_extractor(ppt_stem, aggregate_type, feat_name)\n",
    "        \n",
    "        # Load the mask\n",
    "        mask = nib.load('%s/%s_MNI_1mm.nii.gz' % (mask_path, ROI)).get_data()            \n",
    "        \n",
    "        plt.figure()\n",
    "        bins_all = np.arange(-10, 10.5, 0.5)\n",
    "        hist_vals_all = np.zeros((len(files), len(bins_all) - 1))\n",
    "        prop_sig = []\n",
    "        mean_val = []\n",
    "        ppts = []\n",
    "        for file_counter, file in enumerate(files):\n",
    "\n",
    "            # Load the data\n",
    "            nii = nib.load(file)\n",
    "            vol = nii.get_data()\n",
    "            \n",
    "            if aggregate_type.find('tstat') == -1:\n",
    "                \n",
    "                # Get the ppt name\n",
    "                ppt = file[file.rfind('/s') + 1:file.find('.nii.gz')]\n",
    "                if ppt.find('functional') > -1:\n",
    "                    ppt = ppt[:ppt.rfind('_')] # Take all characters before the last underscore\n",
    "\n",
    "                run_name = file[file.find('functional') + 10:file.find(feat_name) - 1]\n",
    "                \n",
    "            else:\n",
    "                ppt = 'group'\n",
    "                \n",
    "            # Pull out the ppt\n",
    "            ppts += [ppt]\n",
    "\n",
    "            # Apply the mask\n",
    "            vol *= mask \n",
    "\n",
    "            # Get just the voxels in the mask\n",
    "            values = vol[vol != 0] \n",
    "            if  DV.find('proportion_sig_voxels_') >= 0:\n",
    "\n",
    "                thresh_val = float(DV[22:])\n",
    "                # Specify the threshold\n",
    "                prop_sig += [np.mean(values > thresh_val)]\n",
    "\n",
    "                #print('%0.3f' % (prop_sig[-1]))\n",
    "\n",
    "            elif  DV == 'mean':\n",
    "\n",
    "                # Specify the threshold\n",
    "                mean_val += [np.mean(values)]\n",
    "\n",
    "        \n",
    "        # Store all of the participants names\n",
    "        if ROI_counter == 0 and aggregate_type.find('tstat') == -1:\n",
    "            \n",
    "            # Get the participant age\n",
    "            ppt_data = pd.read_csv(preloaded_path + 'participant_age.txt', header=None, sep='\\t', names=['neuropipe_name', 'age', 'cohort'])\n",
    "\n",
    "            included_ages = []\n",
    "            for ppt in ppts:\n",
    "                included_ages += [ppt_data[ppt_data['neuropipe_name'] == ppt]['age'].values[0]]\n",
    "            \n",
    "            df['Ppt_age'] = included_ages\n",
    "\n",
    "        # Wrap up results\n",
    "        if DV.find('proportion_sig_voxels_') >= 0:\n",
    "            df[ROI] = prop_sig\n",
    "            \n",
    "        elif DV == 'mean':\n",
    "            df[ROI] = mean_val            \n",
    "    \n",
    "    #print('Using %d/%d possible files' % (len(ppts), len(files)))\n",
    "    \n",
    "    # If you are returning a dictrionary then do so here\n",
    "    if  DV.find('proportion_sig_voxels_') >= 0:\n",
    "        \n",
    "        # Return the proportion sig values\n",
    "        return df\n",
    "    elif  DV == 'mean':\n",
    "\n",
    "        # Return the proportion sig values\n",
    "        return df\n",
    "    \n",
    "    elif  aggregate_type.find('tstat') >= 0:\n",
    "\n",
    "        # Return the proportion sig values\n",
    "        return prop_sig\n",
    "    \n",
    "def load_each_subject_comparisons(comparisons, aggregate_type, ppt_stem, ROI, DV):\n",
    "    \n",
    "    # Cycle through the ROIs\n",
    "    df = pd.DataFrame()\n",
    "    data = {}\n",
    "    ppts_all = []\n",
    "    for feat_counter, feat_name in enumerate(comparisons):\n",
    "    \n",
    "        # Get the file names\n",
    "        files = file_name_extractor(ppt_stem, aggregate_type, feat_name)\n",
    "        \n",
    "        # Load the mask\n",
    "        mask = nib.load('%s/%s_MNI_1mm.nii.gz' % (mask_path, ROI)).get_data()            \n",
    "        \n",
    "        data[feat_name] = {}\n",
    "        \n",
    "        for file_counter, file in enumerate(files):\n",
    "\n",
    "            # Load the data\n",
    "            nii = nib.load(file)\n",
    "            vol = nii.get_data()\n",
    "            \n",
    "            if aggregate_type.find('tstat') == -1:\n",
    "                \n",
    "                # Check that there are the minimum number of blocks \n",
    "                ppt_run = file[file.rfind('/s') + 1:file.find('.nii.gz')]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                ppt_run = 'group'\n",
    "\n",
    "            # Add this participant and run name\n",
    "            ppts_all += [ppt_run]\n",
    "\n",
    "            # If this is in standard space then apply the mask\n",
    "            vol *= mask \n",
    "\n",
    "            # Get just the voxels in the mask\n",
    "            values = vol[vol != 0] \n",
    "\n",
    "            if  DV.find('proportion_sig_voxels_') >= 0:\n",
    "\n",
    "                thresh_val = float(DV[22:])\n",
    "                # Specify the threshold\n",
    "                data[feat_name][ppt_run] = np.mean(values > thresh_val)\n",
    "\n",
    "                #print('%0.3f' % (prop_sig[-1]))\n",
    "\n",
    "            elif  DV == 'mean':\n",
    "\n",
    "                # Specify the threshold\n",
    "                data[feat_name][ppt_run] = np.mean(values)\n",
    "    \n",
    "    # Remove redundant calls\n",
    "    ppts_all = np.unique(ppts_all)\n",
    "    \n",
    "    # Get the participant age\n",
    "    ppt_data = pd.read_csv(preloaded_path + 'participant_age.txt', header=None, sep='\\t', names=['neuropipe_name', 'age', 'cohort'])\n",
    "\n",
    "    included_ages = []\n",
    "    for ppt_run in ppts_all:\n",
    "        \n",
    "        if ppt_run.find('functional') > -1:\n",
    "            ppt_run = ppt_run[:ppt_run.rfind('_')] # Take all characters before the last underscore\n",
    "\n",
    "        included_ages += [ppt_data[ppt_data['neuropipe_name'] == ppt_run]['age'].values[0]]\n",
    "\n",
    "    df['Ppt_age'] = included_ages\n",
    "    \n",
    "    # Cycle through the conditions and add the participant\n",
    "    \n",
    "    for condition in data.keys():\n",
    "        \n",
    "        feat_data = data[condition]\n",
    "        \n",
    "        # Cycle through the participants. If there is a match then add it, otherwise put a NAN\n",
    "        feat_data_list = []\n",
    "        for ppt in ppts_all:\n",
    "            \n",
    "            if ppt in feat_data.keys():\n",
    "                feat_data_list += [feat_data[ppt]]\n",
    "            else:\n",
    "                feat_data_list += [np.nan]\n",
    "        \n",
    "        # Append the data to the list\n",
    "        df[condition] = feat_data_list\n",
    "                \n",
    "    # Return the proportion sig values\n",
    "    return df\n",
    "\n",
    "def randomise_diff(diff_data, resample_num=10000, return_percentile=False):        \n",
    "    \n",
    "    # Resample the participants\n",
    "    resample_diff = []\n",
    "    for i in range(resample_num):\n",
    "        \n",
    "        # Determine what participants to use in the sample\n",
    "        sub_idx = np.random.randint(0, len(diff_data), (1, len(diff_data)))\n",
    "\n",
    "        resample_diff += [np.mean(diff_data[sub_idx])]\n",
    "    \n",
    "    # If specified, return the percentile\n",
    "    if return_percentile is True:\n",
    "        lower_bound = np.percentile(resample_diff, 2.5)\n",
    "        upper_bound = np.percentile(resample_diff, 97.5)\n",
    "        return lower_bound, upper_bound\n",
    "        \n",
    "    # What direction was the effect\n",
    "    sign_count = np.sum((diff_data) > 0)\n",
    "    \n",
    "    # Calculate the 2 way p value\n",
    "    p_val = (1 - (np.sum(np.asarray(resample_diff) > 0) / (resample_num + 1))) * 2\n",
    "    \n",
    "    # return the difference in ROI and \n",
    "    return p_val, sign_count\n",
    "\n",
    "def run_randomise_stats(df):\n",
    "    \n",
    "    print('V1>A1')\n",
    "    resampled_p_val, sign_count = randomise_diff(df['V1'].values - df['A1'].values)\n",
    "    print('%d/%d show effect, p = %0.3f' % (sign_count, len(df['V1'].values), resampled_p_val))\n",
    "    print('LOC>A1')\n",
    "    resampled_p_val, sign_count = randomise_diff(df['LOC'].values - df['A1'].values)\n",
    "    print('%d/%d show effect, p = %0.3f' % (sign_count, len(df['V1'].values), resampled_p_val))\n",
    "    print('LOC>V1')\n",
    "    resampled_p_val, sign_count = randomise_diff(df['LOC'].values - df['V1'].values)\n",
    "    print('%d/%d show effect, p = %0.3f' % (sign_count, len(df['V1'].values), resampled_p_val))\n",
    "\n",
    "    print('V1>0.05')\n",
    "    resampled_p_val, sign_count = randomise_diff(df['V1'].values - 0.05)\n",
    "    print('M = %0.2f [SD = %0.2f], %d/%d above 0.05, p = %0.3f' % (df['V1'].values.mean(), df['V1'].values.std(), sign_count, len(df['V1'].values), resampled_p_val))\n",
    "    print('LOC>0.05')\n",
    "    resampled_p_val, sign_count = randomise_diff(df['LOC'].values - 0.05)\n",
    "    print('M = %0.2f [SD = %0.2f], %d/%d above 0.05, p = %0.3f' % (df['LOC'].values.mean(), df['LOC'].values.std(), sign_count, len(df['LOC'].values), resampled_p_val))\n",
    "    print('A1>0.05')\n",
    "    resampled_p_val, sign_count = randomise_diff(df['A1'].values - 0.05)\n",
    "    print('M = %0.2f [SD = %0.2f], %d/%d above 0.05, p = %0.3f' % (df['A1'].values.mean(), df['A1'].values.std(), sign_count, len(df['A1'].values), resampled_p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Show wholebrain task based activation for the two cohorts<a id='wholebrain'></a>\n",
    "Load in the whole brain tstat results and plot them according to the threshold. Uses the ROIs specified to trace the ROI contours and overlay them. Does this first for the runwise analyses and then the sessionwise, for both cohorts. The N for each analysis is specified in order to set the p values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.995\n",
    "whole_brain_tstat_dir = preloaded_path + '/wholebrain_tstat/'\n",
    "ROIs = ['V1', 'LOC', 'A1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cohort 1 runwise\n",
    "aggregate_type = 'runwise' \n",
    "cohort = 1\n",
    "filename='%s/cohort_%d_%s.nii.gz' % (whole_brain_tstat_dir, cohort, aggregate_type)\n",
    "\n",
    "# Get the path for this data\n",
    "N = 32 # How many runs are you considering for the t test if there is one\n",
    "\n",
    "# What is the output name for the data\n",
    "output_name = '%s/cohort_%d_%s_%0.3f.svg' % (output_mask_path, cohort, aggregate_type, threshold)\n",
    "\n",
    "plot_sig_map(filename, output_name, threshold, N, ROIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cohort 2 runwise\n",
    "aggregate_type = 'runwise' \n",
    "cohort = 2\n",
    "filename='%s/cohort_%d_%s.nii.gz' % (whole_brain_tstat_dir, cohort, aggregate_type)\n",
    "\n",
    "# Get the path for this data\n",
    "N = 26 # How many runs are you considering for the t test if there is one\n",
    "\n",
    "# What is the output name for the data\n",
    "output_name = '%s/cohort_%d_%s_%0.3f.svg' % (output_mask_path, cohort, aggregate_type, threshold)\n",
    "\n",
    "plot_sig_map(filename, output_name, threshold, N, ROIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cohort 1 sessionwise\n",
    "aggregate_type = 'sessionwise' \n",
    "cohort = 1\n",
    "filename='%s/cohort_%d_%s.nii.gz' % (whole_brain_tstat_dir, cohort, aggregate_type)\n",
    "\n",
    "# Get the path for this data\n",
    "N = 14 # How many runs are you considering for the t test if there is one\n",
    "\n",
    "# What is the output name for the data\n",
    "output_name = '%s/cohort_%d_%s_%0.3f.svg' % (output_mask_path, cohort, aggregate_type, threshold)\n",
    "\n",
    "plot_sig_map(filename, output_name, threshold, N, ROIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cohort 2 sessionwise\n",
    "aggregate_type = 'sessionwise' \n",
    "cohort = 2\n",
    "filename='%s/cohort_%d_%s.nii.gz' % (whole_brain_tstat_dir, cohort, aggregate_type)\n",
    "\n",
    "# Get the path for this data\n",
    "N = 14 # How many runs are you considering for the t test if there is one\n",
    "\n",
    "# What is the output name for the data\n",
    "output_name = '%s/cohort_%d_%s_%0.3f.svg' % (output_mask_path, cohort, aggregate_type, threshold)\n",
    "\n",
    "plot_sig_map(filename, output_name, threshold, N, ROIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot bar plots of ROIs significant voxels<a id='rois'></a>\n",
    "For each ROI in each participant report the number of voxels that exceed a z value corresponding to p = 0.05 (one-tailed). Plot these for each participant, coded by age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_name = 'default' \n",
    "p_val = 0.95 # What is the cut off you want to use\n",
    "z_val = stats.norm.ppf(p_val)\n",
    "DV = 'proportion_sig_voxels_%0.2f' % z_val\n",
    "column_plot = 0 # Do you want to use a column plot or a bee swarm\n",
    "y_range = [0.0, 0.3]\n",
    "y_ticks = [0, 0.1, 0.2, 0.3] # What are the y ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_number = 1 # Cohort 1 or 2\n",
    "aggregate_type = 'all' # Use sessionwise ('concat') or runwise ('all')\n",
    "\n",
    "# Get the ROI values\n",
    "df = load_each_subject(feat_name, aggregate_type, cohort_number, ['V1', 'LOC', 'A1'], DV)\n",
    "\n",
    "plot_bar(df, [V1_color, LOC_color, A1_color])\n",
    "plt.savefig(output_mask_path + 'cohort_1_%s_ROI_comparison.svg' % aggregate_type)\n",
    "\n",
    "# Plot the line graph by age\n",
    "plot_column(df)\n",
    "plt.savefig(output_mask_path + 'cohort_1_%s_ROI_line.svg' % aggregate_type)\n",
    "\n",
    "# Run all the relevant stats\n",
    "run_randomise_stats(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_number = 2\n",
    "aggregate_type = 'all' # Use sessionwise ('concat') or runwise ('all')\n",
    "\n",
    "# Get the ROI values\n",
    "df = load_each_subject(feat_name, aggregate_type, cohort_number, ['V1', 'LOC', 'A1'], DV)\n",
    "\n",
    "plot_bar(df, [V1_color, LOC_color, A1_color])\n",
    "plt.savefig(output_mask_path + 'cohort_2_%s_ROI_comparison.svg' % aggregate_type)\n",
    "\n",
    "# Plot the line graph by age\n",
    "plot_column(df)\n",
    "plt.savefig(output_mask_path + 'cohort_2_%s_ROI_line.svg' % aggregate_type)\n",
    "\n",
    "# Run all the relevant stats\n",
    "run_randomise_stats(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the concatenated analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_number = 1\n",
    "aggregate_type = 'concat' # Use sessionwise ('concat') or runwise ('all')\n",
    "\n",
    "# Get the ROI values\n",
    "df = load_each_subject(feat_name, aggregate_type, cohort_number, ['V1', 'LOC', 'A1'], DV)\n",
    "\n",
    "plot_bar(df, [V1_color, LOC_color, A1_color])\n",
    "plt.savefig(output_mask_path + 'cohort_1_%s_ROI_comparison.svg' % aggregate_type)\n",
    "\n",
    "# Plot the line graph by age\n",
    "plot_column(df)\n",
    "plt.savefig(output_mask_path + 'cohort_1_%s_ROI_line.svg' % aggregate_type)\n",
    "\n",
    "# Run all the relevant stats\n",
    "run_randomise_stats(df)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cohort_number = 2\n",
    "aggregate_type = 'concat' # Use sessionwise ('concat') or runwise ('all')\n",
    "\n",
    "# Get the ROI values\n",
    "df = load_each_subject(feat_name, aggregate_type, cohort_number, ['V1', 'LOC', 'A1'], DV)\n",
    "\n",
    "plot_bar(df, [V1_color, LOC_color, A1_color])\n",
    "plt.savefig(output_mask_path + 'cohort_2_%s_ROI_comparison.svg' % aggregate_type)\n",
    "\n",
    "# Plot the line graph by age\n",
    "plot_column(df)\n",
    "plt.savefig(output_mask_path + 'cohort_2_%s_ROI_line.svg' % aggregate_type)\n",
    "\n",
    "# Run all the relevant stats\n",
    "run_randomise_stats(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore preprocessing parameters<a id='explore'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the preprocessing comparison analyses. For each type of feat run, this sets up a comparison (listed below) and then reports bar plots for each feat (ignoring NaNs) and plot lines connecting a run/session across conditions. \n",
    "\n",
    "This python code cannot not compute the statistics because special packages are needed. Instead, use the text file outputs of this (currently commented out) and refer to these files in the `stats_preprocessing.R` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = [['MotionConfounds_fslmotion_thr0.5', 'MotionConfounds_fslmotion_thr1', 'MotionConfounds_fslmotion_thr3', 'MotionConfounds_fslmotion_thr6', 'MotionConfounds_fslmotion_thr12'],\n",
    "             ['default', 'Motion_recovery_1', 'Motion_recovery_2'],\n",
    "             ['smoothing_0', 'smoothing_3', 'smoothing_5', 'smoothing_8'],\n",
    "             ['MELODIC_thresh_0.25_fslmotion_thr3', 'MELODIC_thresh_0.5_fslmotion_thr3', 'MELODIC_thresh_1.00_fslmotion_thr3'],\n",
    "             ['Temporal_derivative', 'default'],\n",
    "             ['Despiking_None', 'default'],\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cohort_number = 1\n",
    "aggregate_type = 'all' # Use sessionwise ('aggregate') or runwise ('all')\n",
    "\n",
    "y_range = [-0.05, 0.8]\n",
    "y_ticks = [0, 0.25, 0.5, 0.75] # What are the y ticks\n",
    "\n",
    "colors = [V1_color, LOC_color, A1_color]\n",
    "for ROI_counter, ROI in enumerate(ROIs):\n",
    "\n",
    "    # Cycle through the conditions\n",
    "    \n",
    "    for comparison in comparisons:\n",
    "\n",
    "        df = load_each_subject_comparisons(comparison, aggregate_type, cohort_number, ROI, DV)\n",
    "        \n",
    "        # Make a string of the comparisons\n",
    "        comparison_str = ''\n",
    "        for condition in comparison:\n",
    "            comparison_str += condition + '_' \n",
    "        \n",
    "        print('%s%s' % (comparison_str, ROI))\n",
    "        plot_df_line(df, colors[ROI_counter])\n",
    "        \n",
    "        # Set the ylim depending on the type\n",
    "        if 'proportion_sig_voxels' in DV:\n",
    "            plt.ylim(y_range)\n",
    "            plt.yticks(y_ticks)\n",
    "            plt.ylabel('Prop. sig task vs rest')\n",
    "            \n",
    "        plt.savefig(output_mask_path + 'cohort_1_%s%s_comparison.svg' % (comparison_str, ROI))\n",
    "        \n",
    "        # Save data for R stats\n",
    "        df.to_csv(output_mask_path + 'cohort_1_%s%s_comparison.txt' % (comparison_str, ROI), index=False, na_rep='NaN')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this on the concat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cohort_number = 1\n",
    "aggregate_type = 'concat' # Use sessionwise ('concat') or runwise ('all')\n",
    "\n",
    "y_range = [-0.05, 1]\n",
    "y_ticks = [0, 0.25, 0.5, 0.75, 1.0] # What are the y ticks\n",
    "\n",
    "colors = [V1_color, LOC_color, A1_color]\n",
    "for ROI_counter, ROI in enumerate(ROIs):\n",
    "\n",
    "    # Cycle through the conditions\n",
    "    \n",
    "    for comparison in comparisons:\n",
    "\n",
    "        df = load_each_subject_comparisons(comparison, aggregate_type, cohort_number, ROI, DV)\n",
    "        \n",
    "        # Make a string of the comparisons\n",
    "        comparison_str = ''\n",
    "        for condition in comparison:\n",
    "            comparison_str += condition + '_' \n",
    "        \n",
    "        print('%s%s' % (comparison_str, ROI))\n",
    "        plot_df_line(df, colors[ROI_counter])\n",
    "        \n",
    "        # Set the ylim depending on the type\n",
    "        if 'proportion_sig_voxels' in DV:\n",
    "            plt.ylim(y_range)\n",
    "            plt.yticks(y_ticks)\n",
    "            plt.ylabel('Prop. sig task vs rest')\n",
    "    \n",
    "        plt.savefig(output_mask_path + 'cohort_1_concat_%s%s_comparison.svg' % (comparison_str, ROI))\n",
    "        \n",
    "        # Save data for R stats\n",
    "        df.to_csv(output_mask_path + 'cohort_1_concat_%s%s_comparison.txt' % (comparison_str, ROI), index=False, na_rep='NaN')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
